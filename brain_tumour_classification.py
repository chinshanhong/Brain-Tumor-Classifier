# -*- coding: utf-8 -*-
"""Brain Tumour Classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LPZGsWtDlAx02t4SqvcGiNqX7wd-71g5
"""

from google.colab import drive
drive.mount('/content/drive')

"""Import Necessary Modules


"""

# Commented out IPython magic to ensure Python compatibility.
import cv2
import os
import sklearn
import numpy as np
import seaborn as sns
import tensorflow as tf

from tqdm import tqdm
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
from tensorflow.keras.applications import Xception
from tensorflow.keras.callbacks import ReduceLROnPlateau, TensorBoard, ModelCheckpoint
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# %matplotlib inline

# Base path an labels
base_path = '../'
class_names = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']

# Constants
IMAGE_SIZE = 150
BATCH_SIZE = 12
VERBOSE = 1

"""Reading the dataset"""

x_train = []
y_train = []

# Loading Training Datasets from folders
for i in class_names:
  folderPath = os.path.join("/content/drive/MyDrive/UM Files/Year 2 Sem 2/WIX3001 Soft Computing/training brain/", i)
  for j in tqdm(os.listdir(folderPath), ncols = 70):
    img = cv2.imread(os.path.join(folderPath, j))
    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))
    x_train.append(img)
    y_train.append(i)
print('Training dataset loading complete.')

# Loading Testing Datasets from folders
for i in class_names:
  folderPath = os.path.join("/content/drive/MyDrive/UM Files/Year 2 Sem 2/WIX3001 Soft Computing/Testing brain/", i)
  for j in tqdm(os.listdir(folderPath), ncols = 70):
    img = cv2.imread(os.path.join(folderPath, j))
    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))
    x_train.append(img)
    y_train.append(i)
print('\nTesting dataset loading complete.')

x_train = np.array(x_train)
y_train = np.array(y_train)
x_train, y_train = sklearn.utils.shuffle(x_train, y_train, random_state = 0)

sns.countplot(y_train)
plt.show()

"""
Sample of images in each class"""

j = 0
for i in class_names:
  j = 0
  while True:
    if y_train[j] == i:
      plt.figure(figsize = (5, 5))
      plt.imshow(x_train[j])
      plt.title(y_train[j])
      break
    j+=1

aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15, width_shift_range=0.2, height_shift_range=0.2
                         , shear_range=0.15, horizontal_flip=True, fill_mode="nearest")

# Split them into testing and training data
x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, random_state=47, test_size=0.1)
print("Shapes X : Train :", x_train.shape, " Test :", x_test.shape)
print("Shapes Y : Train :", y_train.shape, " Test :", y_test.shape)

"""Encoding the y_train and y_test to numbers"""

y_train_new = [class_names.index(i) for i in y_train]
y_train = y_train_new
y_train = tf.keras.utils.to_categorical(y_train)

y_test_new = [class_names.index(i) for i in y_test]
y_test = y_test_new
y_test = tf.keras.utils.to_categorical(y_test)

# Using the Xception Model
inputShape = (IMAGE_SIZE, IMAGE_SIZE, 3)
xception = Xception(weights='imagenet', input_shape=inputShape, include_top=False)

# Adding the Layers of Neural Network
model = xception.output
model = tf.keras.layers.GlobalAveragePooling2D()(model)
model = tf.keras.layers.Dropout(0.5)(model)
model = tf.keras.layers.Dense(4, activation='softmax')(model)
model = tf.keras.models.Model(inputs=xception.input, outputs=model)

"""Model summary"""

# Model summary
model.summary()

# Creating callbacks
tensorboard = TensorBoard(log_dir='log')
checkpoint = ModelCheckpoint("xception.h5", monitor='val_accuracy', save_best_only=True, mode='auto', verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.3, patience=2, min_delta=0.001, mode='auto', verbose=VERBOSE)

"""Training the model"""

# Compiling and fitting the model
model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])
history = model.fit(aug.flow(x_train, y_train, batch_size=BATCH_SIZE),
                    validation_data=(x_test, y_test), steps_per_epoch=len(x_train) // BATCH_SIZE,
                    epochs=12, callbacks=[reduce_lr, checkpoint, tensorboard])

epochs = [i for i in range(12)]
fig, ax = plt.subplots(1, 2)
train_acc = history.history['accuracy']
train_loss = history.history['loss']
val_acc = history.history['val_accuracy']
val_loss = history.history['val_loss']
fig.set_size_inches(14, 7)

ax[0].plot(epochs, train_acc, 'go-', label = 'Training Accuracy')
ax[0].plot(epochs, val_acc, 'ro-', label = 'Validation Accuracy')
ax[0].set_title("Training & Validation Accuracy")
ax[0].legend()
ax[0].set_xlabel('Epochs')
ax[0].set_ylabel('Accuracy')

ax[1].plot(epochs, train_loss, 'g-o', label = 'Training Loss')
ax[1].plot(epochs, val_loss, 'r-o', label = 'Validation Loss')
ax[1].set_title("Training & Validation Loss")
ax[1].legend()
ax[1].set_xlabel('Epochs')
ax[1].set_ylabel('Training & Validation Loss')

"""Model performance"""

# Evaluating Model
result = model.evaluate(x_test, y_test)
print("Testing Loss :", result[0])
print("Testing Accuracy :", result[1]*100, "%")

predictions = model.predict(x_test)
predictions = np.argmax(predictions, axis = 1)
y_test_edit = np.argmax(y_test, axis = 1)

"""Classification Report"""

cf_report = sklearn.metrics.classification_report(y_test_edit, predictions)
print(cf_report)

"""Confusion Matrix"""

cf_matrix = sklearn.metrics.confusion_matrix(y_test_edit, predictions)
sns.heatmap(cf_matrix, cmap='Blues', linewidth=1, annot=True, xticklabels=class_names, yticklabels=class_names)